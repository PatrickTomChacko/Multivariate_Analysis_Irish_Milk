---
title: "Multivariate Analysis"
author: "Patrick Tom Chacko 22200149"
date: "2023-03-21"
output:
  html_document: default
  pdf_document: default
---
```{r}
library('e1071')
```

```{r}
set.seed(22200149)                      
file <- read.csv('Milk_MIR_Traits_data_2023.csv')     #read in dataset
N <- nrow(file)                                      #Number of rows in our file
del_obs <- sample(1:N,1)                       #sample a random integer here 310
file <- file[-del_obs,]                          #Delete that random integer row 
del_obs
```

310th observation is deleted from our record. Now Let us remove observations with missing values in beta lactoglobulin B. First I extract all the column names and then find the format of required column name, then I use that string to do my latter analysis.

```{r}
#colnames(file)   #to find corresponing column name
#beta_lactoglobulin_b     # required column name
# We will now replace any missing values with NA
file_n <- file[!is.na(file$beta_lactoglobulin_b),] #identify missing rows in beta_l_b column and then remove those rows from our object file
nrow(file_n)
sum(is.na(file$beta_lactoglobulin_b))
```
```{r}
#We will also reindex our dataframe as some observations have been removed
#row.names(file_n) <-1 :nrow(file_n)
```
Using the Not (!) logical operator I have removed the missing values and using the dimensions of our result we can confirm that there were 430 rows in our previous dataset.

```{r}
ncol(file_n) - 531           #Total columns - Spectral columns = non spectral
spectra <- file_n[,52:582]   #Extracting columns of containing spectrum  
spectra_names <- as.numeric( gsub("X", "", colnames(spectra) )) # extract wavelength number for plotting by replacing all X, now spectrum is in corresponding wavelength (integer)
```
Now we will make a convenient dataframe because it is easier to plot
```{r}
easy_df <-  data.frame(file_n$beta_lactoglobulin_b,spectra)
colnames(easy_df) <- c('beta_lac_b',spectra_names)
```
Now we need to divide our observations such that we can utilise the maximum number of plotting characters available at a stretch, if we don't divide -> then pch being less than number of columns will recycle.

We need to plot our data but directly plotting will not help us differentiate so let us look at our beta_lac_b values and we can decide the bin size from there
```{r}
summary(file_n$beta_lactoglobulin_b)
```
I expect similiar beta values to have similiar wavelengths that's why plan on this approach to subset my data, I plan to take the intervals of beta_lactoglobulin_b (beta) as
[0,3),[3,6),[6,9.702]
```{r}
temp1 <- subset(easy_df,(easy_df$beta_lac_b<3))
temp2 <- subset(easy_df,((easy_df$beta_lac_b>=3)&(easy_df$beta_lac_b<6)))
temp3 <- subset(easy_df,(easy_df$beta_lac_b>=6))

nrow(temp1)+nrow(temp2)+nrow(temp3)      #verified our observations our correctly divided
```


```{r}
par(mfrow = c(1,3))
matplot(t(temp1[,-1]),x = spectra_names,pch=1:25, xlim = c(940,3825),ylim = c(-0.2,0.7), main = "Beta in [0,3)")
matplot(t(temp2[,-1]),x = spectra_names,pch=1:25, xlim = c(940,3825),ylim = c(-0.2,0.7), main = "Beta in [3,6)")
matplot(t(temp3[,-1]),x = spectra_names,pch=1:25, xlim = c(940,3825),ylim = c(-0.2,0.7), main = "Beta in [6,9.7)")

```
Plotting the graph as such we can see that observations with beta_lac_b values falling in [0,3) have much variation than others, we can see lots of non overlapping regions for examples at x near 1300,2200 and so on, wheras for other beta_lac_b values we can see the sprectrum returns similar result and huge overlapping makes the plot look dense following the same pattern. So if we are looking for explanatory variables I would go for beta with values that explains much variation.

Let us standardise our data with mean 0 and standard deviation 1

```{r}
mean_easy_df <- apply(easy_df,2,mean)     #find the mean along columns
sd_easy_df <- apply(easy_df,2,sd)         #find sd along columns
std_mean_easy_df <- sweep(easy_df,2,mean_easy_df,'-')    #subtract respective means from col
std_easy_df <- sweep(std_mean_easy_df,2,sd_easy_df,'/')     #divide respective cols by their sd
```

Now we have standardised our data, let us remove all occurences in which abs(beta_lac_b)>3.

```{r}
std_easy_df <- subset(std_easy_df,abs(std_easy_df$beta_lac_b)<=3*sd(std_easy_df$beta_lac_b))
summary(std_easy_df$beta_lac_b)
#row.names(std_easy_df) <- 1:nrow(std_easy_df)
```
So we have 303 observations after the preprocessing. 

#### 3. Hierarchial clustering and k-Means clustering

```{r}
df.eucl <- dist(std_easy_df,method = "euclidean")
#c1.average <- hclust(df.eucl,method = 'average')
#plot(c1.average, xlab = "Average Linkage")
```
```{r}
#c1.single <- hclust(df.eucl,method = 'single')
#plot(c1.single, xlab = "Single Linkage")

```
```{r}
c1.complete <- hclust(df.eucl,method = 'complete')
plot(c1.complete, xlab = "complete Linkage")

```
Looking at the three linkage plot I decide to go with Complete linkage as we can see the heights on y-axis counting from 1 cluster - n clusters is huge resembling huge gap between the clusters, so dissimilar objects are far away.


Further I observe if I cut my y- axis at 55unit I see 5 clusters which have nodes going down of more or less decent width.


```{r}
hcl = cutree(c1.complete, k=5)
table(hcl)
```
We can see that 3 groups have good amount of observations in it.

#### K-Means clustering 

We will calculate the within sum of squares result for values of k between 1 to 10 and the k giving us lowest within sum of squares can be considered as optimal number of clusters and then compare with adjusted Rand index.
```{r}
WGSS = rep(0,10)    #placeholder fr within sum of square
n = nrow(std_easy_df[,-1])
WGSS[1] = (n-1) * sum(apply(std_easy_df[,-1], 2, var))   # WSS = sum(variances of points to their centroids, if n =1 then the mean is the mean of the data)

for(k in 2:10){
WGSS[k] = sum(kmeans(std_easy_df[,-1], centers = k)$withinss)
}

plot(1:10, WGSS, type="b", xlab="k", ylab="Within group sum of squares", main = "WSS vs clusters")
```
Here we observe that k = 3 is the elbow of this curve, we can further verify this with Adjusted rand index


So now we have two values as candidates for optimal clusters {3,5}, we will run tests to see which satisfies maximum of our constraints. We can take a loop for k 1 to 10 and check the rand Index for each of it.

```{r}
hcl = matrix(NA,nrow = 303, ncol = 9)    #make placeholders for clustering result
k_m = matrix(NA,nrow = 303, ncol = 9)
rand <- rep(NA, 9)                # initialising vector for adjusted random index
library(e1071)

  for (i in 2:10){ 
  hcl[,i-1] <- cutree(c1.complete, k = i)   #calculate cluster allocation using hierachial clustering 
  k_m [,i-1]<- kmeans(std_easy_df[,-1], centers = i)$cluster   #calculate cluster allocation using k means
  rand[i-1] <- classAgreement(table(k_m[,i-1],hcl[,i-1]))$crand   #find randIndex using cross Tab agree for each k
  }

plot(2:10,rand, main = "Adjusted rand index for clusters", ylab = "Adj Rand index", xlab = "Clusters")
```
As we can see here the maximum adjusted rand index here appears for k = 5 therefore let's select 5 clusters as optimal number of clusters.

```{r}
set.seed(22200149)
k = 5
k_5 = kmeans(std_easy_df[,-1], center=5)
plot(std_easy_df[,c(2,70,100,178,200,349)],col = k_5$cluster, main = "Clusters in the data using 5 random spectrum")
```
We can see here that the 5 clusters appear to be non overlapping (and single linkage would have had chaining effect, its better not used) , so in the context it would mean that if we know what range of beta_lactoglobulin_b is preferred, that many clusters would be accepted and then tracing it back to the cows/farms which produce it they can be preferred over others.

Overall we can see the data is having high collinearity, so the dimension reduction can be very successful as we would require fewer variables that explain the same variation.


  
```{r}
k_5_df <- as.data.frame(k_5$cluster)
```
Now say one of the clusters is important for us, say cluster 3 here,
```{r}
k_5_df_imp <- subset(k_5_df,k_5_df$`k_5$cluster`==3)  #we subset the rows with cluster 3

des_rows <- row.names(k_5_df_imp)
des_file <- file[des_rows,1:5]
table(des_file$Breed)
```
Here we can see the desired breed would be Hol Fri for the given cluster
```{r}
table(des_file$Milking_Time)
```
Cows with 1 milking are preferred for this activity
```{r}
#table(des_file$Date_of_sampling)   # no conclusion here
#table(des_file$DaysInMilk) #no conclusion
table(des_file$Parity)
```
Parity upto 5 is seen common for this group.


#### Principal Component Analysis
We use prcomp for numerical stability
```{r}
fit = prcomp(std_easy_df[,-1])    #303 x 531 dimension
#names(fit)
```

Let us plot the cummulative proportion of variance explained by the first 10 principal components, as we know that eigen values, principal components, variance are highly inter-related so we can use the standard deviation output to prepare our table. Dividing the value of first principal component by the sum of all principal components (or eigen values) gives us the proportion of variation explained. Mind here the standard deviation is given so need to square it to equate to variation and hence eigen values.

```{r}
s <- (fit$sdev[1:10])^2/sum((fit$sdev)^2)     #finding variation from eigenvalues
sum = rep(0,10)
sum[1] = s[1]

for (i in 2:10) {
  sum[i] = s[i] + sum[i-1]
  
}
plot(1:10,sum, type = 'b', col = 'blue', ylab = "Cummulative prop sum", xlab = "Principal Components", main="Total Variation explained by Principal Components")
```

```{r}
#summary(fit)
```
From here we can see that approximately 99% of variation in data is explained by taking first 5 principal components, these values can be referred from the cumulatve proportion table of fit or even the plot shows that after 4 Principal Components almost 98% variation is explained by the components, so 4-5 principal components can be considered. Even though there is not much difference in jumping from 4 to 5 I would like to take 4 principal components as there is not much change for going from 4 PC to 5 PC.

#### Scores


```{r}
p <- predict(fit,std_easy_df) #Let us store the results to compare with our numerically derived solution

data_temp <- scale(std_easy_df[,-1],center = TRUE, scale = FALSE) #We need to scale our data as fit$scale = FALSE

# dim(data_temp)      check if the dimensions match
# dim(fit$rotation)

pq <- data_temp%*%fit$rotation   #using lecture notes Principle components * Variables == Scores

all(p==pq)
```
We can see its TRUE, signifying all the entries match and we have stored the result in pq, hence we have derived the scores for our spectra now let us plot our principal components, from previous section we can see that 4 principle components are sufficient enough to give us the general picture of our dataset, so lets plot the first 4 principal components.


```{r}
pairs(pq[,1:4], main = "Pairs plot of first 4 principal Components")
```
We can see that there is still some collinearity in PC1 and PC2, since there exists a linear negative relationship here.


#### PCR 

##### Purpose
The purpose of PCR as the name suggests is to use a combination of Principal component Analysis and Regression together to make a better prediction that is computationally efficient and accurate enough. The key idea is to capture the principal Components that explain most of the variation and then use them as predictors in linear regression model and use least squares to estimate our co-efficients and get a respone variable.


##### Description

If our dataset X has n x p as its dimension then the principal component analysis will return us the principal components out of which q of them are sufficient enough to explain a vast range of variation of the model where q << p, then we can think of this as highly computationally efficient mode to understand our data.

Now once we have the new n x q matrix that explains our data (equivalently the first q principal components), then we can feed this as input to our multiple linear regression model as the predictor variables and then fit the best line using least squares which gives us the relationship between the response variable and the q-Principal Components.


##### Model Assumptions/Choices
PCR does better in the cases where first few principal components explain a vast amount of variation in data


##### Advantages and Disadvantages
Advantages are that instead of regressing a n x p matrix we are just regressing n x q where q << p, this is going to save us a lot of computational cost.   

Disadvantages
Even though PCR does better at some cases it cannot be considered as dimension reduction/ feature scaling technique, as the loadings matrix created still need all the variables in actual dataset to multiply and get the scores.

#### PCR - PLS

```{r}
#install.packages('pls')
set.seed(22200149)
library('pls')
row_names_std <- row.names(std_easy_df)  # store in case
N = nrow(std_easy_df)  #total rows in our dataset
train_i =  2/3*(N)   # take 2/3 of orginal data fr training
test_i = N -train_i
train <- sample(1:N,train_i)  #randomly sampling 202 points

test <- setdiff(1:N,train)
#any(train==test)   returns False hence no duplicates

train_data <- std_easy_df[train,]  #subset training data
test_data <- std_easy_df[test,]
```

```{r}
#summary(fit)
```
From approximately 3 Principal Components 95% of variation in data is being explained, so let us consider the first 3 components
```{r}
PCR_res <- pcr(beta_lac_b~., ncomp = 10, data = train_data, validation = "LOO") #regressing beta_lac values from our 3 principal components
summary(PCR_res)
```
We can see the summary giving us cummulative variance explained. And using our Leave One OUT cross validation we have obtained the following results, lets plot it . The validation results are root mean squared error of prediction.
```{r}
plot(RMSEP(PCR_res), legendpos = "topright")
```

Our plot says that 4 components are sufficient to give us a good idea of our data, the lowest RMSEP occurs at n =4.

Let us look at our data closely with 4 components

```{r}
plot(PCR_res, ncomp = 4, asp = 1, line = TRUE)
```

Looking at the figure with aspect ratio 1, we can see that our data is scattered around the line with no clear pattern.
```{r}
plot(PCR_res, plottype = "scores", comps = 1:4)
```
We can see that Component 4 explains very few percentage of variation in the data so if it can be ignored if later we have to drop components. Overall random behaviour of datapoints, except PC1 and PC2 show some linear relation.

Now let us predict our beta_lactoglobulin_B values of our test dataset
```{r}
y_pred <-predict(PCR_res, ncomp = 4, newdata = test_data)
RMSEP(PCR_res, newdata = test_data)
```

We can see that our predictions are close , as for training at 4th component we had 0.94 and now the prediction RMSEP is 0.90 which is not very bad.

#### Imputing using PCA
```{r}
milk_proteins <- file_n[,7:13] #Our data set with 7 milk proteins
row_index_milk <- row.names(milk_proteins)
#row.names(milk_proteins) <- 1:nrow(file_n)
```

```{r}
beta0 <- milk_proteins[milk_proteins$beta_lactoglobulin_b==0,] # we can see there are 32 rows in which beta values after the missings values are removed.
names0 <- row.names(beta0)   #store the row names of 0 values in df0
milk_proteins[names0, "beta_lactoglobulin_b"] <- mean(milk_proteins$beta_lactoglobulin_b)

milk_fit <- prcomp(milk_proteins)   #Principle Components of milk prot
summary(milk_fit)
```

```{r}
mu = colMeans(milk_proteins)
ncomp = 5  # I am taking 5 components into account for better accuracy

milk_hat <- milk_fit$x[,1:ncomp]%*%t(milk_fit$rotation[,1:ncomp])
milk_hat = scale(milk_hat,center = -mu, scale = FALSE)
head(milk_hat)
head(milk_proteins)
```
```{r}
milk_proteins2 <- milk_proteins 
mu <- colMeans(milk_proteins)
mssold <-   mean(apply(((milk_hat - milk_proteins2)[,1:6])^2,2, mean))+ mean(((milk_hat - milk_proteins2)[-as.numeric(names0),7])^2)
```
```{r}
thresh <- 1e-7
rel_err <- 1
iter <- 0
```



```{r}
while (rel_err > thresh) {
# milk_proteins2[names0, "beta_lactoglobulin_b"] <- mean(milk_hat_df[,7])

  iter <- iter + 1
 # Step 2(a)
milk_hat <- prcomp(milk_proteins2)   #PCA
 # Step 2(b) 
milk_hat_df <- milk_fit$x[,1:5]%*%t(milk_fit$rotation[,1:5])  #Reconstructing original mmatrix

 # Step 2(c)
milk_hat_df = scale(milk_hat_df,center = -mu, scale = FALSE)
 

mss <- mean(apply(((milk_hat_df - milk_proteins2)[,1:6])^2,2, mean))  + mean(((milk_hat_df - milk_proteins2)[-as.numeric(names0),7])^2)  #Finding mean squared error seperately for non missing values 

 rel_err <- (mssold - mss) / mssold    #Relative error
 mssold <- mss

milk_proteins2[names0,7] <- milk_hat_df[names0,7] 
 
  cat (" Iter :", iter , " MSS :", mss ,
 " Rel . Err :", rel_err , "\n")
}

```
Here we see that our algorithm has given low Mean square error values, so now lets take the new non missing values as values returned from PCA.

```{r}
milk_proteins2[names0,7]
```

We can see all of them have shifted from the Mean of columns 2.43 giving better approximation that 0 valued data.

#### Predicting using PCR

Let us first make a function that can be helpful for computing our same calculation with differing datasets.

```{r}
PCR_reg <- function(dataset){
  #dataset should have 1st column for target and rest for predictors
 N= nrow(dataset) 
 row_names <- row.names(dataset)
 train_i =  round(2/3*(N))   # take 2/3 of orginal data fr training
 test_i = N -train_i
 train <- sample(row_names,train_i)  #randomly sampling train_i points
 test <- setdiff(row_names,train)  #test data indexes
 
 dataset <- scale(dataset,center = TRUE, scale = TRUE) #scaled data
 dataset <- data.frame(dataset)
 train_data <- dataset[train,]  #subset training data
 test_data <- dataset[test,]
 
 PCR_result <- pcr(beta_lactoglobulin_b ~., ncomp = 10, data = train_data, validation = "LOO") #regressing beta_lac values from our 10 principal components
 
 y_pred <-predict(PCR_result, ncomp = 10, newdata = test_data)
 
 predplot(PCR_result, ncomp = 1:5)
 
 
 df_ <- data.frame(y_pred,test_data$beta_lactoglobulin_b)
 df_$MSE <- (df_[,1]-df_[,2])^2
 mean_MSE <- mean(df_$MSE)
 return(df_)
 }

```

Prepare our datasets 
Initially let us split our data into 2 sets, the first one contains the non zero beta_lactoglobulin_b values.
```{r}
#file_n contains non missing values
data_0 <- subset(file_n, file_n$beta_lactoglobulin_b==0)
data_non0 <- subset(file_n, file_n$beta_lactoglobulin_b!=0)
```
a) predict beta values for data_non0

```{r}
data_non0 <- data_non0[,c(13,52:582)]
non0 <- PCR_reg(data_non0)
```


Here we can see as the principal components increase the overall data seems to get up from the horizontal direction and starts tilting towards slope of 45 degreee which is the absolute requirement here.
```{r}
mean(non0$MSE)
```
We can see here we get a Mean Squared error of 0.893, but mind here we don't have the full dataset into consideration only the ones with non zero beta_lactoglobulin_b values.


b) imputed with mean, Let's take the whole dataset and the 0 values are imputed with mean and then further conduct the analysis
```{r}
data_0$beta_lactoglobulin_b <- mean(file_n[,13])
data_0 <- data_0[,c(13,52:582)]
data_set_imp_mean <- rbind(data_0,data_non0)

mean0 <- PCR_reg(data_set_imp_mean)
```
We can see that after 3rd Principal Component our dataset have started turning towards the Y = X line which is the required case.  
```{r}
mean(mean0$MSE)
```
We can see here a reduction in MSE from 0.893 to 0.70 when we add the Missing at Random Values with the mean imputed values of that column.

c)
Here we make use of our precious section where we had already calculated the beta_lac 0 values using principal components.
The dataframe here is called milk_proteins2.

```{r}
#any(milk_proteins2$beta_lactoglobulin_b==0) gives FALSE
rowdata_imp_pca <- row.names(milk_proteins2)

data_set_imp_pca = data.frame()
data_set_imp_pca[rowdata_imp_pca,1:6] <- file_n[rowdata_imp_pca,1:6]
data_set_imp_pca[rowdata_imp_pca,7:13]<- milk_proteins2[rowdata_imp_pca,]
data_set_imp_pca[rowdata_imp_pca,14:582] <- file_n[rowdata_imp_pca,14:582]
data_set_pca <- data_set_imp_pca[,c(13,53:582)]
```
```{r}
mean_pca <- PCR_reg(data_set_pca)
```
```{r}
mean(mean_pca$MSE)
```
We see a significant reduction in Mean Square error from both part a) and part b) Looking at our predplot also shows us that our Principal Component 1:5 plot shows us a strong collinearity between our predicted and actual values. Thus we can conclude if we have missing values in our dataset, using Principal Components Analysis to find the required imputation is highly considerable before doing PCR. 
